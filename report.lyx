#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\author{Chris Swetenham (s1149322)}
\title{Peer-to-Peer Human-Robot Interaction in Collaborative Tasks}
\date{November 9, 2011}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Survey of Collaborative Problem-Solving in Human-Robot Interaction
\end_layout

\begin_layout Author
Chris Swetenham (s1149322)
\end_layout

\begin_layout Date
19th Jan 2012
\end_layout

\begin_layout Abstract
Collaborative Problem-Solving brings together many different fields in computer
 science and artificial intelligence to enable humans and robots to collaborate
 on shared tasks.
 We investigate the challenges which need to be overcome, and detail several
 recent projects which start the work of combining all the required elements
 for true human-robot collaboration.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Problem Statement
\end_layout

\begin_layout Standard
Collaborative Problem-Solving looks at a class of task in which human and
 robot participants must work together 
\begin_inset Quotes eld
\end_inset

shoulder to shoulder
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2004"

\end_inset

 to cooperatively solve a problem, often using natural speech and gestures
 to communicate.
 This is a multidisciplinary problem in Human-Robot Interaction which brings
 together much of the current work in robotics, natural language, human-computer
 interaction, and artificial intelligence.
 While many parts of the required capabilities have been studied for some
 time, it is only relatively recently that these have been brought together
 into research platforms which attempt to investigate the entire end-to-end
 task of collaborative problem-solving.
\end_layout

\begin_layout Standard
This problem is important because it is how we interact with other humans
 and how we ultimately want to interact with intelligent, embodied agents.
 It encompasses tasks involving teams comprising members with different
 knowledge and skills, both human and robot.
\end_layout

\begin_layout Standard
It is a difficult problem, because it requires many capabilities which have
 no well-established solution, such as gesture recognition, or mature ones
 which are still error-prone, such as speech recognition.
 
\end_layout

\begin_layout Subsection
Challenges
\begin_inset CommandInset label
LatexCommand label
name "sub:Challenges"

\end_inset


\end_layout

\begin_layout Standard
Human and robot participants must be able to understand each other without
 impediment to the task, must understand each other's roles and intentions
 in the task (
\emph on
intention recognition
\emph default
), and must agree on steps to be performed to complete the task (
\emph on
joint intention
\emph default
).
 The robots must also be careful not to take actions which might endanger
 the human participants (
\emph on
safety
\emph default
).
 The robot may need to understand human speech (
\emph on
natural language processing
\emph default
) and make itself understood in return (
\emph on
natural language generation
\emph default
).
 This may require being able to reason about the knowledge and perpective
 of other participants (
\emph on
cognitive modelling 
\emph default
and 
\emph on
perspective taking
\emph default
).
 Speech output can be accompanied by facial expressions or gestures, such
 as looking at and/or pointing to the object being referenced (
\emph on
multimodal interaction
\emph default
).
\end_layout

\begin_layout Subsection
Related Work
\end_layout

\begin_layout Standard
Human-Agent Interaction is an extension of Human-Computer Interaction to
 interactions with software that exhibits some form of agency, potentially
 embodied and situated in a virtual world.
 Human-Robot Interaction is an extension of Human-Agent Interaction to interacti
ons with embodied, situated agents in the physical world.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Goodrich2007"

\end_inset

 gives a survey of the field of Human-Robot Interaction.
\end_layout

\begin_layout Standard
Social Human-Robot Interaction studies robots which interact by social means
 with humans and each other, including gestures, facial expressions and
 speech.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2003"

\end_inset

 gives a survey of Social Human-Robot Interaction.
\end_layout

\begin_layout Standard
Human-Robot Collaboration studies social interaction between humans and
 robots in the context of a shared task to be performed.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bauer2008"

\end_inset

 gives a survey of Human-Robot Collaboration.
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
We draw from the literature a number of motivating examples for cooperative
 problem-solving.
\end_layout

\begin_layout Standard
The first example is drawn from the JAST (Joint-Action Science and Technology)
 Project.
 In this project, a human and a robot collaborate to assemble structures
 from a toy construction kit.
 Both participants communicate using natural language and to a lesser extent
 using gestures.
 The robot can perform some steps of the assembly, instruct the human in
 the assembly steps required, and correct the human participant if they
 pick up the wrong piece.
 [TODO]
\end_layout

\begin_layout Standard
The second example is drawn from the Leonardo Project.
 In this project, a small, highly-expressive robot and a human collaborate
 to activate a sequence of buttons.
 The human communicates with speech and gesture, and the robot entirely
 with gestures.
 The human teaches the robot to press a button, after which the human and
 the robot negotiate with each other the steps in pressing several buttons
 in turn.
 [TODO]
\end_layout

\begin_layout Standard
The third example is a simulated seam-welding exercise as part of the NASA
 Robonaut Project.
 This exercise simulates a team of astronauts and robots assembling a structure
 on a planetary surface.
 Rather than using real welding tools, the exercise uses spray paint for
 ease of experimentation.
 Two astronauts act as 
\begin_inset Quotes eld
\end_inset

master welders
\begin_inset Quotes erd
\end_inset

, placing panels and performing initial 
\begin_inset Quotes eld
\end_inset

tack welds
\begin_inset Quotes erd
\end_inset

.
 One additional astronaut acts as a remote supervisor.
 The highly articulated Robonaut handles a welding tool to finish the welds,
 and a mobile rover robot inspects the quality of the welds.
 When necessary, humans and robots can request additional assistance from
 each other with their individual tasks; for example, the astronauts can
 request the inspection rover to turn its spotlight towards the panel they
 are working on, or the inspector robot can request the advice of the supervisor
 if it cannot determine the quality of a weld.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2006"

\end_inset

 
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
In this report we will cover several projects in the area of collaborative
 problem-solving in human-robot interaction in teams with both human and
 robot participants.
 We will in examine each of the challenges listed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Challenges"

\end_inset

 in turn, and see how they are tackled by the projects listed above, as
 well as in related projects where applicable, including focus on approaches
 to natural language generation, intention recognition and the use natural
 conversation in shared planning and execution of a task.
\end_layout

\begin_layout Standard
In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Human-Robot-Communication"

\end_inset

 we will look at Human-Robot communication in general.
 In Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Natural-Language-Processing"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gesture-Recognition-and"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Multimodal-Interaction"

\end_inset

 we will look at different types of communication.
 In Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Collaborative-Problem-Solving-Dialogue"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Learning"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Mixed-Initiative-Interaction"

\end_inset

 we will look at how the collaborative problem-solving aspect comes together.
 Finally in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"

\end_inset

 we will discuss and summarise.
\end_layout

\begin_layout Section
Human-Robot Communication
\begin_inset CommandInset label
LatexCommand label
name "sec:Human-Robot-Communication"

\end_inset


\end_layout

\begin_layout Standard
At the core of Human-Robot Communication, is the notion of having multiples
 
\emph on
modes
\emph default
 of interaction or communication.
 These interactions can be generated from or interpreted to an internal
 grammar by the robot system.
 The 'sentences' in the grammar are operated on by a dialog engine, which
 interacts with the task planning system and the robot's internal model
 of the world.
\end_layout

\begin_layout Standard
In terms of communication from the robot to the human, there is some leeway
 in which input and output modalities are provided to the robot.
 The Leonardo project
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004"

\end_inset

 focuses on an expressive robot capable of many facial and body gestures
 and without any natural language generation or speech synthesis capabilites.
 The JAST project [TODO] combines two industrial arms with the iCat expressive
 robot face, and features speech generation and recognition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanBreemen2004"

\end_inset

 describes the Lino and iCat robots and the principles used to animate them.
 Museum guide robots such as INDIGO typically feature speech but not gesture
 modes of communication [TODO] although the Robotinho project does use gestures
 [TODO].
\end_layout

\begin_layout Section
Natural Language Processing and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Natural-Language-Processing"

\end_inset


\end_layout

\begin_layout Standard
In general, Speech Recognition and Generation are treated as solved problems
 in human-robot interaction, with some leeway for the inevitable interpretation
 errors from either participant.
 Speech is then treated in terms of text.
 Natural Language Processing is a well established field dealing with processing
 the text into some more abstract model.
 Natural Language Generation is the counterpart of Natural Language Processing.
 It refers to the production of human text or speech from some internal
 data structure specific to the agent.
 In the context of this review the most important aspects are communicating
 the actions to be performed and the objects they should be performed on.
 In fact, generating descriptions of objects and their location is easy;
 the difficult part is deciding which information to include or not in the
 description.
 If there is not enough information, the description can be ambiguous or
 confusing for the human listener; if the information is too specific, it
 may seem to the listener as though the extra information was included for
 a specific reason even if it is irrelevant to the task.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

 the JAST project has investigated different strategies for referring expression
s and their impact on both task performance and user satisfaction.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Byron2009"

\end_inset

 the GIVE challenge evaluates different strategies for guiding a user in
 an online game to move through an environment and perform actions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Belz2008"

\end_inset

 find that task performance metrics do not correlate with metrics that measure
 how 
\begin_inset Quotes eld
\end_inset

human-like
\begin_inset Quotes erd
\end_inset

 the output of an NLG system is.
\end_layout

\begin_layout Section
Gesture Recognition and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Gesture-Recognition-and"

\end_inset


\end_layout

\begin_layout Standard
When humans communicate in a shared task they will often use gestures as
 well as speech.
 These may serve several purposes: to convey intention or emotion, to direct
 the other participant's actions or attention, or to directly execute a
 step in the plan for the task.
 For example, a pointing gesture could direct the other participant's attention
 to a tool or area, or it could tell them to move themselves to the location
 pointed to.
\end_layout

\begin_layout Standard
In a robot, gesture recognition will be performed as a step after the visual
 interpretation of a scene from video data.
\end_layout

\begin_layout Subsection
Hand-over of objects
\end_layout

\begin_layout Standard
Extending an arm holding an object towards the other participants signals
 the intention to hand over the object while also being the first step of
 the execution of this intention.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Koay2006"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Huber2008"

\end_inset

 explore handing-over gestures and approaches that are comfortable and recognisa
ble to the human user.
\end_layout

\begin_layout Subsection
Intent Recognition
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Nehaniv2005"

\end_inset

 looks at classifying human gestures and recognising intent, and find that
 the context as well as the gesture itself need to be taken into account.
 The JAST robot can infer the intent of the human user when they pick up
 pieces and correct them when they pick up the wrong piece 
\begin_inset CommandInset citation
LatexCommand cite
key "Rickert2008"

\end_inset

.
\end_layout

\begin_layout Section
Multimodal Interaction
\begin_inset CommandInset label
LatexCommand label
name "sec:Multimodal-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When performing a task, humans have many simultaneous modes of interaction:
 speech, facial expressions and stances, gaze direction, gestures, direct
 physical contact.
 It may be beneficial or even necessary to consider multiple modes at once
 to understand the overall meaning.
\end_layout

\begin_layout Standard
In the JAST project, the generation of referring expressions and the use
 of gestures were both studied and evaluated for their effectiveness in
 communicating with the human participant [TODO].
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanDerSluis"

\end_inset

 gives an algorithm for the generation of multimodal referring expressions.
 In the Leonardo project, speech output was avoided entirely in favour of
 an expressive body and facial design.
 [TODO] describes the MultiML language for representing multimodal actions
 in a dialogue.
\end_layout

\begin_layout Section
Collaborative Problem-Solving Dialogue 
\begin_inset CommandInset label
LatexCommand label
name "sec:Collaborative-Problem-Solving-Dialogue"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Larsson2000"

\end_inset

 describes the TRINDI dialogue engine toolkit.
 It is based on the notion of a shared or individual 
\emph on
information state
\emph default
 which is updated by the 
\emph on
dialog moves
\emph default
 of the participants.
 The tookit defines the basic data structures and some dialog moves, but
 the precise information and the choice of dialog moves can be selected
 according to the task required.
 Simple examples of dialog moves are asking or answering a question.
 The TRINDI toolkit has been used in the JAST project.
\end_layout

\begin_layout Standard
A similar toolkit, Ariadne, is used by the NASA HRI/OS.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2006"

\end_inset

 describes NASA's HRI/OS, developed as part of the Robonaut robotic astronaut
 project.
 HRI/OS allows for a wide range of interactions between humans and robots,
 from remote teleoperation to local collaboration.
 Robots using this system can request help from humans or other robots when
 they are unable to complete a task by themselves.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 propose a different system which includes dialogue moves (which they call
 
\emph on
interaction acts
\emph default
) which are used to negotiate, accept, or reject changes to the shared problem-s
olving state, such as deciding to focus on a particular subproblem or adopt
 a certain solution.
 In addition, there are wrapped in 
\emph on
grounding acts
\emph default
 which handle turn-taking in conversation, requests for acknowledgement,
 and requests for clarification.
\end_layout

\begin_layout Standard
TODO - move?
\end_layout

\begin_layout Standard
Joint Intention refers to the state of affairs where several participants
 share a common goal and a common plan for achieving that goal.
 In order to reach this situation, both participants must continually communicat
e their intentions as the execution of the task progresses.
 In the above system, joint intention is achieved by negotiating every change
 to a shared problem-solving state.
\end_layout

\begin_layout Standard
TODO - move?
\end_layout

\begin_layout Standard
Within the scope of collaborative problem solving, each member of the team,
 human or robot, can take on different roles.
 One example is the related roles of teacher and student.
\end_layout

\begin_layout Standard
In the Leonardo project, the human participant sets the goal and teaches
 the robot the steps required [TODO].
 For example, the robot is taught to press a button, and then is given the
 task of pressing several buttons.
\end_layout

\begin_layout Standard
In the reverse direction, in the JAST project, the robot participant sets
 the goal and teaches the human the steps required [TODO].
 The human is taught sub-tasks which are then combined into a larger task.
\end_layout

\begin_layout Section
Learning
\begin_inset CommandInset label
LatexCommand label
name "sec:Learning"

\end_inset


\end_layout

\begin_layout Standard
In terms of working on a shared task, the Leonardo project focuses on the
 human teaching the robot how to participate in a task.
 The idea is that active tutelage can be much more effective than relying
 on blind experimentation by the robot or complex, brittle a priori knowledge
 in getting the robot to perform a new task.
\end_layout

\begin_layout Section
Mixed-Initiative Interaction
\begin_inset CommandInset label
LatexCommand label
name "sec:Mixed-Initiative-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When both participants contribute towards the goal, this is termed a Mixed-Initi
ative system.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Guinn1996"

\end_inset

 describes an early model of mixed-initiative communication based on exchanging
 information and deductions between agents until a conclusion is reached.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004b"

\end_inset

 describe how Leonardo can suggest it takes the initiative or request help
 from its human partner.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 describe a model for negotiating shared goals and plans between participants,
 and use it to analyse a planning discussion between two human participants.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferguson2007"

\end_inset

 describe an architecture for implementing a similar model in an agent.
\end_layout

\begin_layout Section
Anticipation
\begin_inset CommandInset label
LatexCommand label
name "sec:Anticipation"

\end_inset


\end_layout

\begin_layout Standard
Beyond the projects listed so far, some recent work has investigated the
 potential benefits of the robot participants anticipating the actions and
 needs of the human participants.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2008"

\end_inset

 investigates a joint task between a human and a robotic lamp under time
 pressure.
 They found that TODO.
 TODO 
\begin_inset CommandInset citation
LatexCommand cite
key "Shah2011"

\end_inset

.
\end_layout

\begin_layout Section
Safety
\begin_inset CommandInset label
LatexCommand label
name "sec:Safety"

\end_inset


\end_layout

\begin_layout Standard
If humans and robots are to collaborate in close proximity, safety is an
 important issue
\begin_inset CommandInset citation
LatexCommand cite
key "Alami2006"

\end_inset

.
 In traditional industrial robots, humans and robots are simply kept separated
 by distance or physical barriers and have a kill switch to stop the robot
 entirely in an emergency.
 This is the approach taken in the JAST project, where the robot only interacts
 with its own side of the table, except to hand an object to the human participa
nt.
\end_layout

\begin_layout Standard
The second version of the Robonaut project tackled the issue of safe interaction
 directly, with a triply-redundant system using feedback from motors to
 monitor and limit the forces applied by the robot to its environment.
 As a result, collisions with a human or with another unexpected obstacle
 will simply stop or slow the motion of the robot to safe levels
\begin_inset CommandInset citation
LatexCommand cite
key "Diftler2011"

\end_inset

.
\end_layout

\begin_layout Section
Evaluation Methods
\begin_inset CommandInset label
LatexCommand label
name "sec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
Evaluation of collaborative problem-solving tasks can be difficult since
 there are many possible metrics, and some of the interesting ones can be
 expensive to gather since they require human annotation of captured data.
 It is straightforward to capture video and audio data of the task performance,
 as well as tracking internal state of the robot and measuring the task
 success or failure and the time taken for the task.
 Others, such as the number of errors made or other properties of the dialog,
 may require human annotation.
\end_layout

\begin_layout Standard
The evaluation methods used in collaborative problem-solving can be traced
 back to ones used in Human-Computer Interaction and Natural Language Generation
, for example the PARADISE evaluation framework
\begin_inset CommandInset citation
LatexCommand cite
key "Walker1997"

\end_inset

.
\end_layout

\begin_layout Standard
In practice, evaluation is based on several or all of: task success or failure,
 time taken to complete the task, rate of errors based on human annotation
 of session recordings, and human participant satisfaction based on standard
 questionnaires
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2005,Foster2009a,Shah2011"

\end_inset

.
\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
Collaborative Problem-Solving is a dynamic and promising area of Human-Robot
 Interaction research, with applications in space
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2005"

\end_inset

, medical and elderly care
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2003"

\end_inset

, and any context in which it is desirable to have autonomous robot participants
 perform tasks in a dangerous environment in collaboration with human participan
ts, such as military applications and urban search and rescue
\begin_inset CommandInset citation
LatexCommand cite
key "Goodrich2007"

\end_inset

.
 Collaborative problem-solving, mixed-intiative and multimodal interactions
 allow for natural integration of robot members in a team without requiring
 special training of the human participants, and as the robot becomes more
 aware of intent and social cues, it is regarded less as a tool and more
 as a participant
\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2007b"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "References/IRR"
options "amsalpha"

\end_inset


\end_layout

\end_body
\end_document
