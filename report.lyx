#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\author{Chris Swetenham (s1149322)}
\title{Peer-to-Peer Human-Robot Interaction in Collaborative Tasks}
\date{November 9, 2011}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% Gestures, facial expressions, natural language, physical interaction/touch
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Recognising and exhibiting emotions, intention
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Learning and respecting social convention
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Mixed-initiative interaction - not fixed turns in conversation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Conversation, speech recognition and generation, natural language processing
 and generation.
 Generating referring expressions
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Nasa robonaut
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Modelling what the other can see and giving and understanding references
 to objects in their frame of reference
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Safety
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% Online games for evaluating and learning, data collection
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Human-Robot Interaction in collaborative tasks is a multidisciplinary problem
 which brings together much of the current work in robotics, natural language,
 human-computer interaction, and artificial intelligence.
 Human and robot participants must be able to understand each other without
 impediment to the task, must understand each other's roles and intentions
 in the task, and must agree on steps to be performed to complete the task.
 The robots must also be careful not to take actions which might endanger
 the human participants.
\end_layout

\begin_layout Standard
When performing a task, humans have many simultaneous modes of interaction:
 speech, facial expressions and stances, gaze direction, gestures, direct
 physical contact.
 These may serve several purposes: to convey intention or emotion, to direct
 the other participant's actions or attention, or to directly execute a
 step in the plan for the task.
 For example, a pointing gesture could direct the other participant's attention
 to a tool or area, or it could tell them to move themselves to the location
 pointed to.
 Extending an arm holding an object towards the other participants signals
 the intention to hand over the object while also being the first step of
 the execution of this intention.
 It may be be necessary to consider multiple modes at once to understand
 the overall meaning.
\end_layout

\begin_layout Standard
In this report we will cover several projects in the area of collaborative
 problem-solving in human-robot interaction.
 We will in particular focus on approaches to natural language generation,
 intention recognition and the use natural conversation in shared planning
 and execution of a task.
\end_layout

\begin_layout Section
TODO
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Projects:
\end_layout

\begin_layout Plain Layout
Lino->iCat
\end_layout

\begin_layout Plain Layout
iCat + AC -> JAST -> JAMES
\end_layout

\begin_layout Plain Layout
Kismet -> Leonardo
\end_layout

\begin_layout Plain Layout
INDIGO
\end_layout

\begin_layout Plain Layout
Paro
\end_layout

\begin_layout Plain Layout
Robonaut -> Robonaut 2
\end_layout

\begin_layout Plain Layout
Build core of lit review around the software architectures used in each
 project? And maybe some various strategies for error-handling, etc scenarios?
\end_layout

\begin_layout Plain Layout
GIVE challenge: can give history of entries, strategies, performance
\end_layout

\begin_layout Plain Layout
Areas: intention, nlg, cps
\end_layout

\begin_layout Standard
Intro: describe the general task and some of the projects covered.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "References/refs"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
