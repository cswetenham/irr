#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\author{Chris Swetenham (s1149322)}
\title{Peer-to-Peer Human-Robot Interaction in Collaborative Tasks}
\date{November 9, 2011}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Collaborative Problem-Solving in Human-Robot Interaction
\end_layout

\begin_layout Author
Chris Swetenham (s1149322)
\end_layout

\begin_layout Date
19th Jan 2012
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Problem Statement
\end_layout

\begin_layout Standard
Collaborative Problem-Solving is a multidisciplinary problem in Human-Robot
 Interaction which brings together much of the current work in robotics,
 natural language, human-computer interaction, and artificial intelligence.
 In this class of task, human and robot participants must work together
 
\begin_inset Quotes eld
\end_inset

shoulder to shoulder
\begin_inset Quotes erd
\end_inset

[TODO] to cooperatively solve a problem.
 While many parts of the required capabilities have been studied for some
 time, it is only relatively recently that these have been brought together
 into research platforms which attempt to investigate the entire end-to-end
 task of collaborative problem-solving.
\end_layout

\begin_layout Subsection
Requirements
\end_layout

\begin_layout Standard
Human and robot participants must be able to understand each other without
 impediment to the task, must understand each other's roles and intentions
 in the task (
\emph on
intention recognition
\emph default
), and must agree on steps to be performed to complete the task (
\emph on
joint intention
\emph default
).
 The robots must also be careful not to take actions which might endanger
 the human participants (
\emph on
safety
\emph default
).
 Speech output can be accompanied by facial expressions or gestures, such
 as looking at and/or pointing to the object being referenced.
 This an example of 
\emph on
multimodal interaction
\emph default
.
\end_layout

\begin_layout Subsection
Area
\end_layout

\begin_layout Standard
Human-Agent Interaction is an extension of Human-Computer Interaction to
 interactions with software that exhibits some form of agency, potentially
 embodied and situated in a virtual world.
 Human-Robot Interaction is an extension of Human-Agent Interaction to interacti
ons with embodied, situated agents in the physical world.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Goodrich2007"

\end_inset

 gives a survey of the field of Human-Robot Interaction.
\end_layout

\begin_layout Standard
Social Human-Robot Interaction studies robots which interact by social means
 with humans and each other, including gestures, facial expressions and
 speech.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2003"

\end_inset

 gives a survey of Social Human-Robot Interaction.
\end_layout

\begin_layout Standard
Human-Robot Collaboration studies social interaction between humans and
 robots in the context of a shared task to be performed.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bauer2008"

\end_inset

 gives a survey of Human-Robot Collaboration.
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
We draw from the literature a number of motivating examples for cooperative
 problem-solving.
\end_layout

\begin_layout Standard
The first example is drawn from the JAST (Joint-Action Science and Technology)
 Project.
 In this project, a human and a robot collaborate to assemble structures
 from a toy construction kit.
 Both participants communicate using natural language and to a lesser extent
 using gestures.
 The robot can perform some steps of the assembly, instruct the human in
 the assembly steps required, and correct the human participant if they
 pick up the wrong piece.
 [TODO]
\end_layout

\begin_layout Standard
The second example is drawn from the Leonardo Project.
 In this project, a small, highly-expressive robot and a human collaborate
 to activate a sequence of buttons.
 The human communicates with speech and gesture, and the robot entirely
 with gestures.
 The human teaches the robot to press a button, after which the human and
 the robot negotiate with each other the steps in pressing several buttons
 in turn.
 [TODO]
\end_layout

\begin_layout Standard
The third example is a simulated seam-welding exercise as part of the NASA
 Robonaut Project.
 This exercise simulates a team of astronauts and robots assembling a structure
 on a planetary surface.
 Rather than using real welding tools, the exercise uses spray paint for
 ease of experimentation.
 Two astronauts act as 
\begin_inset Quotes eld
\end_inset

master welders
\begin_inset Quotes erd
\end_inset

, placing panels and performing initial 
\begin_inset Quotes eld
\end_inset

tack welds
\begin_inset Quotes erd
\end_inset

.
 One additional astronaut acts as a remote supervisor.
 The highly articulated Robonaut handles a welding tool to finish the welds,
 and a mobile rover robot inspects the quality of the welds.
 When necessary, humans and robots can request additional assistance from
 each other with their individual tasks; for example, the astronauts can
 request the inspection rover to turn its spotlight towards the panel they
 are working on, or the inspector robot can request the advice of the supervisor
 if it cannot determine the quality of a weld.
 [TODO Fong2006]
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
In this report we will cover several projects in the area of collaborative
 problem-solving in human-robot interaction between a single robot and single
 human participant.
 We will in particular focus on approaches to natural language generation,
 intention recognition and the use natural conversation in shared planning
 and execution of a task.
\end_layout

\begin_layout Standard
In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Human-Robot-Communication"

\end_inset

 we will look at Human-Robot communication in general.
 In Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Natural-Language-Processing"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gesture-Recognition-and"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Multimodal-Interaction"

\end_inset

 we will look at different types of communication.
 In Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Collaborative-Problem-Solving"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Learning"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Mixed-Initiative-Interaction"

\end_inset

 we will look at how the collaborative problem-solving aspect comes together.
 Finally in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"

\end_inset

 we will discuss and summarise.
\end_layout

\begin_layout Section
Human-Robot Communication
\begin_inset CommandInset label
LatexCommand label
name "sec:Human-Robot-Communication"

\end_inset


\end_layout

\begin_layout Standard
At the core of Human-Robot Communication, is the notion of having multiples
 
\emph on
modes
\emph default
 of interaction or communication.
 These interactions can be generated from or interpreted to an internal
 grammar by the robot system.
 The 'sentences' in the grammar are operated on by a dialog engine, which
 interacts with the task planning system and the robot's internal model
 of the world.
\end_layout

\begin_layout Standard
In terms of communication from the robot to the human, there is some leeway
 in which input and output modalities are provided to the robot.
 The Leonardo project
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004"

\end_inset

 focuses on an expressive robot capable of many facial and body gestures
 and without any natural language generation or speech synthesis capabilites.
 The JAST project [TODO] combines two industrial arms with the iCat expressive
 robot face, and features speech generation and recognition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanBreemen2004"

\end_inset

 describes the Lino and iCat robots and the principles used to animate them.
 Museum guide robots such as INDIGO typically feature speech but not gesture
 modes of communication [TODO] although the Robotinho project does use gestures
 [TODO].
\end_layout

\begin_layout Section
Natural Language Processing and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Natural-Language-Processing"

\end_inset


\end_layout

\begin_layout Standard
In general, Speech Recognition and Generation are treated as solved problems
 in human-robot interaction, with some leeway for the inevitable interpretation
 errors from either participant.
 Speech is then treated in terms of text.
 Natural Language Processing is a well established field dealing with processing
 the text into some more abstract model.
 Natural Language Generation is the counterpart of Natural Language Processing.
 It refers to the production of human text or speech from some internal
 data structure specific to the agent.
 In the context of this review the most important aspects are communicating
 the actions to be performed and the objects they should be performed on.
 In fact, generating descriptions of objects and their location is easy;
 the difficult part is deciding which information to include or not in the
 description.
 If there is not enough information, the description can be ambiguous or
 confusing for the human listener; if the information is too specific, it
 may seem to the listener as though the extra information was included for
 a specific reason even if it is irrelevant to the task.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

 the JAST project has investigated different strategies for referring expression
s and their impact on both task performance and user satisfaction.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Byron2009"

\end_inset

 the GIVE challenge evaluates different strategies for guiding a user in
 an online game to move through an environment and perform actions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Belz2008"

\end_inset

 find that task performance metrics do not correlate with metrics that measure
 how 
\begin_inset Quotes eld
\end_inset

human-like
\begin_inset Quotes erd
\end_inset

 the output of an NLG system is.
\end_layout

\begin_layout Section
Gesture Recognition and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Gesture-Recognition-and"

\end_inset


\end_layout

\begin_layout Standard
When humans communicate in a shared task they will often use gestures as
 well as speech.
 These may serve several purposes: to convey intention or emotion, to direct
 the other participant's actions or attention, or to directly execute a
 step in the plan for the task.
 For example, a pointing gesture could direct the other participant's attention
 to a tool or area, or it could tell them to move themselves to the location
 pointed to.
\end_layout

\begin_layout Standard
In a robot, gesture recognition will be performed as a step after the visual
 interpretation of a scene from video data.
\end_layout

\begin_layout Subsection
Hand-over of objects
\end_layout

\begin_layout Standard
Extending an arm holding an object towards the other participants signals
 the intention to hand over the object while also being the first step of
 the execution of this intention.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Koay2006"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Huber2008"

\end_inset

 explore handing-over gestures and approaches that are comfortable and recognisa
ble to the human user.
\end_layout

\begin_layout Subsection
Intent Recognition
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Nehaniv2005"

\end_inset

 looks at classifying human gestures and recognising intent, and find that
 the context as well as the gesture itself need to be taken into account.
 The JAST robot can infer the intent of the human user when they pick up
 pieces and correct them when they pick up the wrong piece 
\begin_inset CommandInset citation
LatexCommand cite
key "Rickert2008"

\end_inset

.
\end_layout

\begin_layout Section
Multimodal Interaction
\begin_inset CommandInset label
LatexCommand label
name "sec:Multimodal-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When performing a task, humans have many simultaneous modes of interaction:
 speech, facial expressions and stances, gaze direction, gestures, direct
 physical contact.
 It may be beneficial or even necessary to consider multiple modes at once
 to understand the overall meaning.
\end_layout

\begin_layout Standard
In the JAST project, the generation of referring expressions and the use
 of gestures were both studied and evaluated for their effectiveness in
 communicating with the human participant [TODO].
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanDerSluis"

\end_inset

 gives an algorithm for the generation of multimodal referring expressions.
 In the Leonardo project, speech output was avoided entirely in favour of
 an expressive body and facial design.
 [TODO] describes the MultiML language for representing multimodal actions
 in a dialogue.
\end_layout

\begin_layout Section
Collaborative Problem-Solving
\begin_inset CommandInset label
LatexCommand label
name "sec:Collaborative-Problem-Solving"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Larsson2000"

\end_inset

 describes the TRINDI dialog engine toolkit.
 It is based on the notion of a shared or individual 
\emph on
information state
\emph default
 which is updated by the 
\emph on
dialog moves
\emph default
 of the participants.
 The tookit defines the basic data structures and some dialog moves, but
 the precise information and the choice of dialog moves can be selected
 according to the task required.
 Simple examples of dialog moves are asking or answering a question.
 The TRINDI toolkit has been used in the JAST project.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 propose a different system which includes dialogue moves (which they call
 
\emph on
interaction acts
\emph default
) which are used to negotiate, accept, or reject changes to the shared problem-s
olving state, such as deciding to focus on a particular subproblem or adopt
 a certain solution.
 In addition, there are wrapped in 
\emph on
grounding acts
\emph default
 which handle turn-taking in conversation, requests for acknowledgement,
 and requests for clarification.
\end_layout

\begin_layout Standard
Joint Intention refers to the state of affairs where several participants
 share a common goal and a common plan for achieving that goal.
 In order to reach this situation, both participants must continually communicat
e their intentions as the execution of the task progresses.
 In the above system, joint intention is achieved by negotiating every change
 to a shared problem-solving state.
\end_layout

\begin_layout Standard
In the Leonardo project, the human participant sets the goal and teaches
 the robot the steps required [TODO].
 For example, the robot is taught to press a button, and then is given the
 task of pressing several buttons.
\end_layout

\begin_layout Standard
In the reverse direction, in the JAST project, the robot participant sets
 the goal and teaches the human the steps required [TODO].
 The human is taught sub-tasks which are then combined into a larger task.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2006"

\end_inset

 describes NASA's HRI/OS, developed as part of the Robonaut robotic astronaut
 project.
 HRI/OS allows for a wide range of interactions between humans and robots,
 from remote teleoperation to local collaboration.
 Robots using this system can request help from humans or other robots when
 they are unable to complete a task by themselves.
\end_layout

\begin_layout Section
Learning
\begin_inset CommandInset label
LatexCommand label
name "sec:Learning"

\end_inset


\end_layout

\begin_layout Standard
In terms of working on a shared task, the Leonardo project focuses on the
 human teaching the robot how to participate in a task.
 The idea is that active tutelage can be much more effective than relying
 on blind experimentation by the robot or complex, brittle a priori knowledge
 in getting the robot to perform a new task.
\end_layout

\begin_layout Section
Mixed-Initiative Interaction
\begin_inset CommandInset label
LatexCommand label
name "sec:Mixed-Initiative-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When both participants contribute towards the goal, this is termed a Mixed-Initi
ative system.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Guinn1996"

\end_inset

 describes an early model of mixed-initiative communication based on exchanging
 information and deductions between agents until a conclusion is reached.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004b"

\end_inset

 describe how Leonardo can suggest it takes the initiative or request help
 from its human partner.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 describe a model for negotiating shared goals and plans between participants,
 and use it to analyse a planning discussion between two human participants.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferguson2007"

\end_inset

 describe an architecture for implementing a similar model in an agent.
\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
Collaborative Problem-Solving is a dynamic and promising area of Human-Robot
 Interaction research, with applications in space[TODO], medical and elderly
 care[TODO], and any context in which it is desirable to have autonomous
 robot participants perform tasks in a dangerous environment in collaboration
 with human participants, such as military applications[TODO] and urban
 search and rescue[TODO].
 Collaborative problem-solving, mixed-intiative and multimodal interactions
 allow for natural integration of robot members in a team without requiring
 special training of the human participants, and as the robot becomes more
 aware of intent and social cues, it is regarded less as a tool and more
 as a participant[TODO].
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Projects:
\end_layout

\begin_layout Plain Layout
Lino->iCat
\end_layout

\begin_layout Plain Layout
iCat + AC -> JAST -> JAMES
\end_layout

\begin_layout Plain Layout
Kismet -> Leonardo
\end_layout

\begin_layout Plain Layout
INDIGO
\end_layout

\begin_layout Plain Layout
Paro
\end_layout

\begin_layout Plain Layout
Robonaut -> Robonaut 2
\end_layout

\begin_layout Plain Layout
Build core of lit review around the software architectures used in each
 project? And maybe some various strategies for error-handling, etc scenarios?
\end_layout

\begin_layout Plain Layout
GIVE challenge: can give history of entries, strategies, performance
\end_layout

\begin_layout Plain Layout
Talk about safety? Appearance, uncanny valley?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "References/IRR"
options "amsalpha"

\end_inset


\end_layout

\end_body
\end_document
