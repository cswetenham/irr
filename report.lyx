#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\author{Chris Swetenham (s1149322)}
\title{Peer-to-Peer Human-Robot Interaction in Collaborative Tasks}
\date{November 9, 2011}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Survey of Collaborative Problem-Solving in Human-Robot Interaction
\end_layout

\begin_layout Author
Chris Swetenham (s1149322)
\end_layout

\begin_layout Date
19th Jan 2012
\end_layout

\begin_layout Abstract
Collaborative Problem-Solving brings together many different fields in computer
 science and artificial intelligence to enable humans and robots to collaborate
 on shared tasks.
 We investigate the challenges which need to be overcome, and detail several
 recent projects which start the work of combining all the required elements
 for true human-robot collaboration.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Problem Statement
\end_layout

\begin_layout Standard
Collaborative Problem-Solving looks at a class of task in which human and
 robot participants must work together 
\begin_inset Quotes eld
\end_inset

shoulder to shoulder
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2004"

\end_inset

 to cooperatively solve a problem, often using natural speech and gestures
 to communicate.
 This is a multidisciplinary problem in Human-Robot Interaction which brings
 together much of the current work in robotics, natural language, human-computer
 interaction, and artificial intelligence.
 While many parts of the required capabilities have been studied for some
 time, it is only relatively recently that these have been brought together
 into research platforms which attempt to investigate the entire end-to-end
 task of collaborative problem-solving.
\end_layout

\begin_layout Standard
This problem is important because it is how we interact with other humans
 and how we ultimately want to interact with intelligent, embodied agents.
 It encompasses tasks involving teams comprising members with different
 knowledge and skills, both human and robot.
\end_layout

\begin_layout Standard
It is a difficult problem, because it requires many capabilities which have
 no well-established solution, such as gesture recognition, or mature ones
 which are still error-prone, such as speech recognition.
 
\end_layout

\begin_layout Subsection
Challenges
\begin_inset CommandInset label
LatexCommand label
name "sub:Challenges"

\end_inset


\end_layout

\begin_layout Standard
Human and robot participants must be able to understand each other without
 impediment to the task, must understand each other's roles and intentions
 in the task (
\emph on
intention recognition
\emph default
), and must agree on steps to be performed to complete the task (
\emph on
joint intention
\emph default
).
 The robots must also be careful not to take actions which might endanger
 the human participants (
\emph on
safety
\emph default
).
 The robot may need to understand human speech (
\emph on
natural language processing
\emph default
) and make itself understood in return (
\emph on
natural language generation
\emph default
).
 This may require being able to reason about the knowledge and perpective
 of other participants (
\emph on
cognitive modelling 
\emph default
and 
\emph on
perspective taking
\emph default
).
 Speech output can be accompanied by facial expressions or gestures, such
 as looking at and/or pointing to the object being referenced (
\emph on
multimodal interaction
\emph default
).
\end_layout

\begin_layout Subsection
Related Work
\end_layout

\begin_layout Standard
Human-Agent Interaction is an extension of Human-Computer Interaction to
 interactions with software that exhibits some form of agency, potentially
 embodied and situated in a virtual world.
 Human-Robot Interaction is an extension of Human-Agent Interaction to interacti
ons with embodied, situated agents in the physical world.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Goodrich2007"

\end_inset

 gives a survey of the field of Human-Robot Interaction.
\end_layout

\begin_layout Standard
Social Human-Robot Interaction studies robots which interact by social means
 with humans and each other, including gestures, facial expressions and
 speech.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2003"

\end_inset

 gives a survey of Social Human-Robot Interaction.
\end_layout

\begin_layout Standard
Human-Robot Collaboration studies social interaction between humans and
 robots in the context of a shared task to be performed.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bauer2008"

\end_inset

 gives a survey of Human-Robot Collaboration.
\end_layout

\begin_layout Subsection
Motivating Examples
\end_layout

\begin_layout Standard
We draw from the literature a number of motivating examples for cooperative
 problem-solving.
\end_layout

\begin_layout Standard
The first example is drawn from the JAST (Joint-Action Science and Technology)
 Project.
 In this project, a human and a robot collaborate to assemble structures
 from a toy construction kit.
 Both participants communicate using natural language and to a lesser extent
 using gestures.
 The robot can perform some steps of the assembly, instruct the human in
 the assembly steps required, and correct the human participant if they
 pick up the wrong piece
\begin_inset CommandInset citation
LatexCommand cite
key "Rickert2008"

\end_inset

.
\end_layout

\begin_layout Standard
The second example is drawn from the Leonardo Project.
 In this project, a small, highly-expressive robot and a human collaborate
 to activate a sequence of buttons.
 The human communicates with speech and gesture, and the robot entirely
 with gestures.
 The human teaches the robot to press a button, after which the human and
 the robot negotiate with each other the steps in pressing several buttons
 in turn
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004b"

\end_inset

.
\end_layout

\begin_layout Standard
The third example is a simulated seam-welding exercise as part of the NASA
 Robonaut Project.
 This exercise simulates a team of astronauts and robots assembling a structure
 on a planetary surface.
 Rather than using real welding tools, the exercise uses spray paint for
 ease of experimentation.
 Two astronauts act as 
\begin_inset Quotes eld
\end_inset

master welders
\begin_inset Quotes erd
\end_inset

, placing panels and performing initial 
\begin_inset Quotes eld
\end_inset

tack welds
\begin_inset Quotes erd
\end_inset

.
 One additional astronaut acts as a remote supervisor.
 The highly articulated Robonaut handles a welding tool to finish the welds,
 and a mobile rover robot inspects the quality of the welds.
 When necessary, humans and robots can request additional assistance from
 each other with their individual tasks; for example, the astronauts can
 request the inspection rover to turn its spotlight towards the panel they
 are working on, or the inspector robot can request the advice of the supervisor
 if it cannot determine the quality of a weld
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2006"

\end_inset

.
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
In this report we will cover several projects in the area of collaborative
 problem-solving in human-robot interaction in teams with both human and
 robot participants.
 We will in examine each of the challenges listed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Challenges"

\end_inset

 in turn, and see how they are tackled by the projects listed above, as
 well as in related projects where applicable, including focus on approaches
 to natural language generation, intention recognition and the use natural
 conversation in shared planning and execution of a task.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:arch"

\end_inset

 shows the general software architecture for collaborative problem-solving
 robots.
 
\end_layout

\begin_layout Standard
In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Human-Robot-Communication"

\end_inset

 we will look at forms of Human-Robot communication in general.
 In Sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Natural-Language-Processing"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gesture-Recognition-and"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Multimodal-Interaction"

\end_inset

 we will look at different types and stages of communication.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Dialogue"

\end_inset

, we will look at how the collaborative problem-solving aspect comes together
 in the dialogue between the participants.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Evaluation"

\end_inset

 we will examine how performance in collaborative problem-solving tasks
 is evaluated.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Future-Directions"

\end_inset

 we will discuss recent and future directions in collaborative problem-solving.
 Finally in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"

\end_inset

 we will discuss and summarise.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename drawing.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example architecture of a robot's software for collaborative problem-solving
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:arch"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Human-Robot Communication
\begin_inset CommandInset label
LatexCommand label
name "sec:Human-Robot-Communication"

\end_inset


\end_layout

\begin_layout Standard
At the core of Human-Robot Communication, is the notion of having multiples
 
\emph on
modes
\emph default
 of interaction or communication.
 These interactions can be generated from or interpreted to an internal
 grammar by the robot system.
 The 'sentences' in the grammar are operated on by a dialog engine, which
 interacts with the task planning system and the robot's internal model
 of the world.
\end_layout

\begin_layout Standard
In terms of communication from the robot to the human, there is some leeway
 in which input and output modalities are provided to the robot.
 The Leonardo project
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004"

\end_inset

 focuses on an expressive robot capable of many facial and body gestures
 and without any natural language generation or speech synthesis capabilites.
 The JAST project 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2006a"

\end_inset

 combines two industrial arms with the iCat expressive robot face, and features
 speech generation and recognition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanBreemen2004"

\end_inset

 describes the Lino and iCat robots and the principles used to animate them.
\end_layout

\begin_layout Section
Natural Language Processing and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Natural-Language-Processing"

\end_inset


\end_layout

\begin_layout Standard
Speech is processed in several forms in a collaborate problem-solving system.
 The lowest level is raw sound samples captured from or output to the physical
 world.
 The next level is a textual representation, in sentences or sentence fragments.
 The next level after this is more abstract and consists of syntax trees
 representing the information communicated.
 At this level, the representation can be independent of the human language
 used by the system.
 The JAST project, for example, is able to converse in both English and
 German
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

.
 The final level of abstraction is whatever internal representation is used
 by the dialogue engine to represent the state of the dialogue.
\end_layout

\begin_layout Standard
Translation between these representations is handled by different components
 of the system.
 Translation between speech and text is done by Speech Recognition and Text-To-S
peech.
 Translation between text and abstract sentences is done my Natural Language
 Processing and Natural Language Generation.
 Translation between abstract sentences and internal representations is
 done by the Dialogue Manager.
\end_layout

\begin_layout Standard
In general, Speech Recognition and Text-To-Speech are treated as solved
 problems in human-robot interaction, with some leeway for the inevitable
 interpretation errors from either participant.
 It will not be discussed further in this paper.
\end_layout

\begin_layout Standard
Natural Language Processing is a well established field dealing with processing
 the text into some more abstract model.
 It is a fairly well-solved problem in the context of collaborative problem-solv
ing, since it is a restricted domain: the participants will be discussing
 only the shared world they are operating in, and their intentions with
 respect to the task.
\end_layout

\begin_layout Standard
Natural Language Generation is the counterpart of Natural Language Processing.
 It refers to the production of human text or speech from some internal
 data structure specific to the agent.
 In the context of this review the most important aspects are communicating
 the actions to be performed and the objects they should be performed on.
 In fact, generating descriptions of objects and their location is easy;
 the difficult part is deciding which information to include or not in the
 description.
 If there is not enough information, the description can be ambiguous or
 confusing for the human listener; if the information is too specific, it
 may seem to the listener as though the extra information was included for
 a specific reason even if it is irrelevant to the task.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

 the JAST project has investigated different strategies for referring expression
s and their impact on both task performance and user satisfaction.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Byron2009"

\end_inset

 the GIVE challenge evaluates different strategies for guiding a user in
 an online game to move through an environment and perform actions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Belz2008"

\end_inset

 find that task performance metrics do not correlate with metrics that measure
 how 
\begin_inset Quotes eld
\end_inset

human-like
\begin_inset Quotes erd
\end_inset

 the output of an NLG system is.
\end_layout

\begin_layout Standard
The Dialogue Manager is discussed in its own Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Dialogue"

\end_inset

.
\end_layout

\begin_layout Section
Gesture Recognition and Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Gesture-Recognition-and"

\end_inset


\end_layout

\begin_layout Standard
When humans communicate in a shared task they will often use gestures as
 well as speech.
 These may serve several purposes: to convey intention or emotion, to direct
 the other participant's actions or attention, or to directly execute a
 step in the plan for the task.
 For example, a pointing gesture could direct the other participant's attention
 to a tool or area, or it could tell them to move themselves to the location
 pointed to.
\end_layout

\begin_layout Standard
In a robot, gesture recognition will be performed as a step after the visual
 interpretation of a scene from video data.
\end_layout

\begin_layout Subsection
Hand-over of objects
\end_layout

\begin_layout Standard
Extending an arm holding an object towards the other participants signals
 the intention to hand over the object while also being the first step of
 the execution of this intention.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Koay2006"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Huber2008"

\end_inset

 explore handing-over gestures and approaches that are comfortable and recognisa
ble to the human user.
\end_layout

\begin_layout Subsection
Intent Recognition
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Nehaniv2005"

\end_inset

 looks at classifying human gestures and recognising intent, and find that
 the context as well as the gesture itself need to be taken into account.
 The JAST robot can infer the intent of the human user when they pick up
 pieces and correct them when they pick up the wrong piece 
\begin_inset CommandInset citation
LatexCommand cite
key "Rickert2008"

\end_inset

.
\end_layout

\begin_layout Section
Multimodal Interaction
\begin_inset CommandInset label
LatexCommand label
name "sec:Multimodal-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When performing a task, humans have many simultaneous modes of interaction:
 speech, facial expressions and stances, gaze direction, gestures, direct
 physical contact.
 It may be beneficial or even necessary to consider multiple modes at once
 to understand the overall meaning.
\end_layout

\begin_layout Standard
In the JAST project, the generation of referring expressions and the use
 of gestures were both studied and evaluated for their effectiveness in
 communicating with the human participant 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "VanDerSluis"

\end_inset

 gives an algorithm for the generation of multimodal referring expressions.
 In the Leonardo project, speech output was avoided entirely in favour of
 an expressive body and facial design.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Giuliani2008"

\end_inset

 describes the MultiML language for representing multimodal actions in a
 dialogue.
\end_layout

\begin_layout Section
Dialogue 
\begin_inset CommandInset label
LatexCommand label
name "sec:Dialogue"

\end_inset


\end_layout

\begin_layout Standard
Collaborative problem-solving requires a dialogue engine which at minimum
 can navigate through pre-defined dialogues and keep track of the current
 state.
 More advanced dialogue engines include planning and plan or intention recogniti
on components which allow the engine to reason about and react to the intentions
 of other participants.
\end_layout

\begin_layout Standard
Joint Intention refers to the state of affairs where several participants
 share a common goal and a common plan for achieving that goal.
 In order to reach this situation, both participants must continually communicat
e their intentions as the execution of the task progresses.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Larsson2000"

\end_inset

 describes the TRINDI dialogue engine toolkit.
 It is based on the notion of a shared or individual 
\emph on
information state
\emph default
 which is updated by the 
\emph on
dialog moves
\emph default
 of the participants.
 The tookit defines the basic data structures and some dialog moves, but
 the precise information and the choice of dialog moves can be selected
 according to the task required.
 Simple examples of dialog moves are asking or answering a question.
 The TRINDI toolkit has been used in the JAST project.
\end_layout

\begin_layout Standard
A similar toolkit, Ariadne, is used by the NASA HRI/OS.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Denecke2002"

\end_inset

 describes the Ariandne toolkit.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2006"

\end_inset

 describes NASA's HRI/OS, developed as part of the Robonaut robotic astronaut
 project.
 HRI/OS allows for a wide range of interactions between humans and robots,
 from remote teleoperation to local collaboration.
 Robots using this system can request help from humans or other robots when
 they are unable to complete a task by themselves.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Rich2001"

\end_inset

 describes the highly influential COLLAGEN dialogue system for collaborative
 dialogue, developed in the context of collaborative problem-solving with
 a software agent rather than a robot.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 propose a different system which includes dialogue moves (which they call
 
\emph on
interaction acts
\emph default
) which are used to negotiate, accept, or reject changes to the shared problem-s
olving state, such as deciding to focus on a particular subproblem or adopt
 a certain solution.
 In addition, there are wrapped in 
\emph on
grounding acts
\emph default
 which handle turn-taking in conversation, requests for acknowledgement,
 and requests for clarification.
 In this system, joint intention is achieved by negotiating every change
 to a shared problem-solving state.
\end_layout

\begin_layout Standard
The dialogue drives the shared execution of the task, and different ways
 of collaborating lead to different dialogue managers.
 For example, within the scope of collaborative problem solving, each member
 of the team, human or robot, can take on different roles.
 One example is the related roles of teacher and student.
 In the Leonardo project, the human participant sets the goal and teaches
 the robot the steps required
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004b"

\end_inset

.
 For example, the robot is taught to press a button, and then is given the
 task of pressing several buttons.
\end_layout

\begin_layout Standard
In the reverse direction, in the JAST project, the robot participant sets
 the goal and teaches the human the steps required 
\begin_inset CommandInset citation
LatexCommand cite
key "Foster2009a"

\end_inset

.
 The human is taught sub-tasks which are then combined into a larger task.
\end_layout

\begin_layout Section
Evaluation Methods
\begin_inset CommandInset label
LatexCommand label
name "sec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
Evaluation of collaborative problem-solving tasks can be difficult since
 there are many possible metrics, and some of the interesting ones can be
 expensive to gather since they require human annotation of captured data.
 It is straightforward to capture video and audio data of the task performance,
 as well as tracking internal state of the robot and measuring the task
 success or failure and the time taken for the task.
 Others, such as the number of errors made or other properties of the dialog,
 may require human annotation.
\end_layout

\begin_layout Standard
The evaluation methods used in collaborative problem-solving can be traced
 back to ones used in Human-Computer Interaction and Natural Language Generation
, for example the PARADISE evaluation framework
\begin_inset CommandInset citation
LatexCommand cite
key "Walker1997"

\end_inset

.
\end_layout

\begin_layout Standard
In practice, evaluation is based on several or all of: task success or failure,
 time taken to complete the task, rate of errors based on human annotation
 of session recordings, and human participant satisfaction based on standard
 questionnaires
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2005,Foster2009a,Shah2011"

\end_inset

.
\end_layout

\begin_layout Section
Future Directions
\begin_inset CommandInset label
LatexCommand label
name "sec:Future-Directions"

\end_inset


\end_layout

\begin_layout Subsection
Learning
\begin_inset CommandInset label
LatexCommand label
name "sub:Learning"

\end_inset


\end_layout

\begin_layout Standard
In terms of working on a shared task, the Leonardo project focuses on the
 human teaching the robot how to participate in a task.
 The idea is that active tutelage can be much more effective than relying
 on blind experimentation by the robot or complex, brittle a priori knowledge
 in getting the robot to perform a new task.
\end_layout

\begin_layout Subsection
Mixed-Initiative Interaction
\begin_inset CommandInset label
LatexCommand label
name "sub:Mixed-Initiative-Interaction"

\end_inset


\end_layout

\begin_layout Standard
When both participants contribute towards the goal, this is termed a Mixed-Initi
ative system.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Guinn1996"

\end_inset

 describes an early model of mixed-initiative communication based on exchanging
 information and deductions between agents until a conclusion is reached.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Breazeal2004b"

\end_inset

 describe how Leonardo can suggest it takes the initiative or request help
 from its human partner.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Blaylock2005"

\end_inset

 describe a model for negotiating shared goals and plans between participants,
 and use it to analyse a planning discussion between two human participants.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferguson2007"

\end_inset

 describe an architecture for implementing a similar model in an agent.
\end_layout

\begin_layout Subsection
Anticipation
\begin_inset CommandInset label
LatexCommand label
name "sub:Anticipation"

\end_inset


\end_layout

\begin_layout Standard
Beyond the projects listed so far, some recent work has investigated the
 potential benefits of the robot participants anticipating the actions and
 needs of the human participants.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2008"

\end_inset

 investigates a joint task between a human and a robotic lamp under time
 pressure.
 The robotic lamp could be commanded to move by pointing by the human, and
 could be told to change colour.
 The human and robot team had to complete a sequence of actions in the shortest
 time possible.
 They found that they could make the robot learn to anticipate the actions
 of their partner and start moving to help them rather than reacting after
 the fact.
 In some cases, the robot member became so effective that the human participant
 felt they were letting the team down with their own performance.
 This suggests that building anticipation into collaborative systems can
 greatly enhance their performance.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Shah2011"

\end_inset

 applies these same ideas to an assembly task similar to that in the JAST
 project.
 By anticipating the required pieces it could bring them before the human
 participant requested them, significantly reducing the time the human participa
nt spent idle waiting for the robot.
\end_layout

\begin_layout Subsection
Safety
\begin_inset CommandInset label
LatexCommand label
name "sub:Safety"

\end_inset


\end_layout

\begin_layout Standard
If humans and robots are to collaborate in close proximity, safety is an
 important issue
\begin_inset CommandInset citation
LatexCommand cite
key "Alami2006"

\end_inset

.
 In traditional industrial robots, humans and robots are simply kept separated
 by distance or physical barriers and have a kill switch to stop the robot
 entirely in an emergency.
 This is the approach taken in the JAST project, where the robot only interacts
 with its own side of the table, except to hand an object to the human participa
nt.
\end_layout

\begin_layout Standard
The second version of the Robonaut project tackled the issue of safe interaction
 directly, with a triply-redundant system using feedback from motors to
 monitor and limit the forces applied by the robot to its environment.
 As a result, collisions with a human or with another unexpected obstacle
 will simply stop or slow the motion of the robot to safe levels
\begin_inset CommandInset citation
LatexCommand cite
key "Diftler2011"

\end_inset

.
\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
Collaborative Problem-Solving is a dynamic and promising area of Human-Robot
 Interaction research, with applications in space
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2005"

\end_inset

, medical and elderly care
\begin_inset CommandInset citation
LatexCommand cite
key "Fong2003"

\end_inset

, and any context in which it is desirable to have autonomous robot participants
 perform tasks in a dangerous environment in collaboration with human participan
ts, such as military applications and urban search and rescue
\begin_inset CommandInset citation
LatexCommand cite
key "Goodrich2007"

\end_inset

.
\end_layout

\begin_layout Standard
In many cases the scope of the work on collaborative problem-solving is
 limited by the manipulation and visual understanding capabilities of the
 robot, but the nature of the tasks undertaken remains realistic and relavant
 to applications in the real world.
 Collaborative problem-solving, mixed-intiative and multimodal interactions
 allow for natural integration of robot members in a team without requiring
 special training of the human participants, and as the robot becomes more
 aware of intent and social cues, it becomes an efficient collaborator and
 is regarded less as a tool and more as a participant
\begin_inset CommandInset citation
LatexCommand cite
key "Hoffman2007b"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "References/IRR"
options "amsalpha"

\end_inset


\end_layout

\end_body
\end_document
